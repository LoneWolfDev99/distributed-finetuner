apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: "finetune-llama2-"
  namespace: amol
spec:
  entrypoint: main
  serviceAccountName: default-editor
  arguments:
    parameters:
    - name: hf_api_token
      value: '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'
    - name: e2e_tir_access_token
      value: 'eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJGSjg2R2NGM2pUYk5MT2NvNE52WmtVQ0lVbWZZQ3FvcXRPUWVNZmJoTmxFIn0.eyJleHAiOjE3MTc3NDc5NDYsImlhdCI6MTY4NjIxMTk0NiwianRpIjoiM2MwM2NkNTYtNDJkMC00OTc2LTk3ZGItNDdjZTRhYzA3ZjNmIiwiaXNzIjoiaHR0cDovL2dhdGV3YXkuZTJlbmV0d29ya3MuY29tL2F1dGgvcmVhbG1zL2FwaW1hbiIsImF1ZCI6ImFjY291bnQiLCJzdWIiOiI2YTcwNDgzNC1hZmI0LTRlNTYtODM0Yi02NDc5ZDgzM2U0NzEiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhcGltYW51aSIsInNlc3Npb25fc3RhdGUiOiI1M2EyMzFmYi1lZmExLTQwYjMtYTg5Ni01MDNlMzUzZWU5YWYiLCJhY3IiOiIxIiwiYWxsb3dlZC1vcmlnaW5zIjpbIiJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImFwaXVzZXIiLCJkZWZhdWx0LXJvbGVzLWFwaW1hbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsInNpZCI6IjUzYTIzMWZiLWVmYTEtNDBiMy1hODk2LTUwM2UzNTNlZTlhZiIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwibmFtZSI6IkFtb2wgVW1iYXJrYXIiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJhbW9sLnVtYmFya2FyQGUyZW5ldHdvcmtzLmNvbSIsImdpdmVuX25hbWUiOiJBbW9sIiwiZmFtaWx5X25hbWUiOiJVbWJhcmthciIsImVtYWlsIjoiYW1vbC51bWJhcmthckBlMmVuZXR3b3Jrcy5jb20ifQ.ihLB4woQAr7XQ_Px6WK9JqYTjQ8L0smzBda1vgTBgQK79z8Rj0mBahR2x1f8gqKWtAv5lG68o5R-a0rotaDtXN1h78_iKA0qxPhUFjbD5aemAizf-4Z3XKPEf0mr1c8QNpSEIi2Jb1Cgc4cLuFIAQ8Ngr8RYqprau-QrHHsGTuA'
    - name: e2e_tir_api_key
      value: '620ccfe2-7f01-4fbc-bdcc-b1536f6d1c84'
    - name: e2e_tir_project_id
      value: '1445'
    - name: e2e_tir_team_id
      value: '8'
    - name: e2e_tir_finetune_job_id
      value: '2323'
    # run_name should be unique on a per-run basis, especially if reporting
    # to wandb or sharing PVCs between runs.
    - name: run_name
      value: 'llama2'
    - name: pvc
      value: 'home/jovyan'
    # Training parameters. Model IDs are HuggingFace identifiers to pull down,
    # or a path to your model relative to the PVC root.
    - name: model
      value: 'meta-llama/Llama-2-7b-hf'
    - name: dataset_name
      value: 'mlabonne/guanaco-llama2-1k'
    - name: dataset_text_field
      value: 'text'
    - name: dataset_type
      value: 'huggingface' # can be tir-dataset, eos-bucket as well
    - name: dataset_bucket
      value: ''
    - name: dataset_path
      value: ''
    - name: dataset_accesskey 
      value: '' 
    - name: dataset_secretkey 
      value: '' 
    - name: dataset_split
      value: 1
    - name: batch_size
      value: 1
    - name: seq_length
      value: 512
    - name: num_train_epochs
      value: 3
    - name: max_steps
      value: -1
    - name: save_steps
      value: 100
    - name: save_total_limit
      value: 10
    - name: use_peft
      value: 'true'
    - name: peft_lora_r
      value: 64
    - name: peft_lora_alpha
      value: 16
    - name: trust_remote_code
      value: 'false'
    - name: use_auth_token
      value: 'true'
    - name: trainer_ram
      value: 192
    - name: trainer_gpus
      value: 1
    # Training number of CPU cores
    - name: trainer_cores
      value: 8
    - name: torch_cache
      value: "/{{workflow.parameters.pvc}}/torch/"    
    - name: transformers_cache
      value: "/{{workflow.parameters.pvc}}/hf/"    
    - name: finetuner_image
      value: 'amole2e/fine-tuner'
    - name: finetuner_tag
      value: 'latest'
    - name: sku_type
      value: 'gpu-a100'
    # WandB token.
    - name: wandb_key
      value: ''
    - name: wandb_project
      value: ''
    - name: learning_rate
      value: 1.41e-5
    - name: max_train_samples
      value: 10
    - name: max_eval_samples
      value: 10
  templates:
  - name: main
    steps:
    - - name: finetuner
        template: model-finetuner
        arguments:
          parameters:
          - name: finetuner_params
            # Pass the params in as one string so we can template in the conditional flags
            value: >-
              --model_name={{workflow.parameters.model}}
              --dataset_name={{workflow.parameters.dataset_name}}
              --dataset_text_field={{workflow.parameters.dataset_text_field}}
              --dataset_split={{workflow.parameters.dataset_split}}
              --dataset_type={{workflow.parameters.dataset_type}}
              --dataset_bucket={{workflow.parameters.dataset_bucket}}
              --dataset_path={{workflow.parameters.dataset_path}}
              --dataset_accesskey={{workflow.parameters.dataset_accesskey}}
              --dataset_secretkey={{workflow.parameters.dataset_secretkey}}
              --output_dir=/{{workflow.parameters.pvc}}/finetunes/
              --num_train_epochs={{workflow.parameters.num_train_epochs}}
              --batch_size={{workflow.parameters.batch_size}}
              --use_peft={{workflow.parameters.use_peft}}
              --peft_lora_r={{workflow.parameters.peft_lora_r}}
              --peft_lora_alpha={{workflow.parameters.peft_lora_alpha}}
              --max_steps={{workflow.parameters.max_steps}}
              --save_steps={{workflow.parameters.save_steps}}
              --save_total_limit={{workflow.parameters.save_total_limit}}
              --max_train_samples={{workflow.parameters.max_train_samples}}
              --max_eval_samples={{workflow.parameters.max_eval_samples}}
          - name: wandb_key
            value: "{{workflow.parameters.wandb_key}}"
          - name: trainer_ram
            value: "{{workflow.parameters.trainer_ram}}"
          - name: trainer_cores
            value: "{{workflow.parameters.trainer_cores}}"
          - name: trainer_gpus
            value: "{{workflow.parameters.trainer_gpus}}"
          - name: torch_cache
            value: "/{{workflow.parameters.pvc}}/torch/"
          - name: tranformers_cache
            value: "/{{workflow.parameters.pvc}}/hf/"
          - name: e2e_tir_access_token
            value: "{{workflow.parameters.e2e_tir_access_token}}"
          - name:  e2e_tir_api_key
            value: "{{workflow.parameters.e2e_tir_api_key}}"
          - name: e2e_tir_project_id
            value: "{{workflow.parameters.e2e_tir_project_id}}"
          - name: e2e_tir_team_id
            value: "{{workflow.parameters.e2e_tir_team_id}}"
          - name: e2e_tir_finetune_job_id
            value: "{{workflow.parameters.e2e_tir_finetune_job_id}}"  
  - name: model-finetuner
    inputs:
      parameters:
        - name: finetuner_params
        - name: wandb_key
        - name: torch_cache
        - name: trainer_ram
        - name: trainer_cores
        - name: trainer_gpus
        - name: tranformers_cache
        - name: e2e_tir_access_token
        - name: e2e_tir_api_key
        - name: e2e_tir_project_id
        - name: e2e_tir_team_id
        - name: e2e_tir_finetune_job_id
    podSpecPatch: |
      containers:
        - name: main
          resources:
            requests:
              memory: "{{workflow.parameters.trainer_ram}}Gi"
              cpu: "{{workflow.parameters.trainer_cores}}"
              nvidia.com/gpu: "{{workflow.parameters.trainer_gpus}}"
              ephemeral-storage: 100Gi
            limits:
              memory: "{{workflow.parameters.trainer_ram}}Gi"
              cpu: "{{workflow.parameters.trainer_cores}}"
              nvidia.com/gpu: "{{workflow.parameters.trainer_gpus}}"
              ephemeral-storage: 100Gi
    container:
      image: "{{workflow.parameters.finetuner_image}}:{{workflow.parameters.finetuner_tag}}"
      command: ["/usr/bin/bash", "-c"]
      args: ["python3 /home/jovyan/finetuner.py {{inputs.parameters.finetuner_params}}"]
      tty: true
      env:
      - name: WANDB_API_KEY
        value: "{{inputs.parameters.wandb_key}}"
      - name: WANDB_LOG_MODEL
        value: "false"
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: TORCH_EXTENSIONS_DIR
        value: "{{inputs.parameters.torch_cache}}"
      - name: HF_API_TOKEN
        value: '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'
      - name: HF_TOKEN
        value: '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'
      - name: TRANSFORMERS_CACHE
        value: "{{inputs.parameters.tranformers_cache}}"   
      - name: E2E_TIR_ACCESS_TOKEN
        value: "{{inputs.parameters.e2e_tir_access_token}}"
      - name: E2E_TIR_API_KEY
        value: "{{inputs.parameters.e2e_tir_api_key}}"
      - name: E2E_TIR_PROJECT_ID
        value: "{{inputs.parameters.e2e_tir_project_id}}"
      - name: E2E_TIR_TEAM_ID
        value: "{{inputs.parameters.e2e_tir_team_id}}"
      - name: E2E_TIR_FINETUNE_JOB_ID
        value: "{{inputs.parameters.e2e_tir_finetune_job_id}}"
      - name: E2E_TIR_API_HOST
        value: ""
      resources:
        requests:
          memory: 100Gi
          cpu: 8
          nvidia.com/gpu: 1
        limits:
          memory: 100Gi
          cpu: 8
          nvidia.com/gpu: 1
      volumeMounts:
        # - mountPath: "/mnt/data"
        #   name: "data"
        - name: dshm
          mountPath: /dev/shm
    volumes:
      # - name: "data"
      #   persistentVolumeClaim:
      #      claimName: "data"
      - emptyDir:
          medium: Memory
        name: dshm
    # affinity:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #         - matchExpressions:
    #             - key: worker_type
    #               operator: In
    #               values:
    #                 - "{{workflow.parameters.sku_type}}"
